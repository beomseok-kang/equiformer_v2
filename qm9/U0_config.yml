trainer: forces_v2


# dataset:
#   - src: datasets/oc20/s2ef/2M/train/
#     normalize_labels: True
#     target_mean: -0.7554450631141663
#     target_std: 2.887317180633545
#     grad_target_mean: 0.0
#     grad_target_std: 2.887317180633545
#   - src: datasets/oc20/s2ef/all/val_id/


# logger: wandb


# task:
#   dataset: trajectory_lmdb_v2
#   description: "Regressing to energies and forces for DFT trajectories from OCP"
#   type: regression
#   metric: force_mae
#   labels:
#     - potential energy
#   grad_input: atomic forces
#   train_on_free_atoms: True
#   eval_on_free_atoms: True
  

hide_eval_progressbar: False


model:
  name: equiformer_v2
  
  use_pbc:                  False
  regress_forces:           False
  otf_graph:                True
  max_neighbors:            500
  max_radius:               5.0
  max_num_elements:         90

  num_layers:               5
  sphere_channels:          96
  attn_hidden_channels:     48              # [64, 96] This determines the hidden size of message passing. Do not necessarily use 96. 
  num_heads:                4
  attn_alpha_channels:      64              # Not used when `use_s2_act_attn` is True. 
  attn_value_channels:      24
  ffn_hidden_channels:      96
  norm_type:                'layer_norm_sh'    # ['rms_norm_sh', 'layer_norm', 'layer_norm_sh']

  lmax_list:                [4]             
  mmax_list:                [4]             
  grid_resolution:          18              # [18, 16, 14, None] For `None`, simply comment this line. 

  num_sphere_samples:       128

  edge_channels:              64
  use_atom_edge_embedding:    True
  share_atom_edge_embedding:  False         # If `True`, `use_atom_edge_embedding` must be `True` and the atom edge embedding will be shared across all blocks. 
  distance_function:          'gaussian'
  num_distance_basis:         512           # not used

  attn_activation:          'silu'
  use_s2_act_attn:          False       # [False, True] Switch between attention after S2 activation or the original EquiformerV1 attention. 
  use_attn_renorm:          True        # Attention re-normalization. Used for ablation study.
  ffn_activation:           'silu'      # ['silu', 'swiglu']
  use_gate_act:             False       # [True, False] Switch between gate activation and S2 activation
  use_grid_mlp:             True        # [False, True] If `True`, use projecting to grids and performing MLPs for FFNs.
  use_sep_s2_act:           True        # Separable S2 activation. Used for ablation study.

  alpha_drop:               0.0         # [0.0, 0.1]
  drop_path_rate:           0.0        # [0.0, 0.05] 
  proj_drop:                0.0

  weight_init:              'uniform'    # ['uniform', 'normal']


optim:
  batch_size:                   48         # 6
  eval_batch_size:              96         # 6
  grad_accumulation_steps:      1         # gradient accumulation: effective batch size = `grad_accumulation_steps` * `batch_size` * (num of GPUs)
  load_balancing: atoms
  num_workers: 8
  lr_initial:                   0.00015    # [0.0002, 0.0004], eSCN uses 0.0008 for batch size 96
  
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.0
  scheduler: linear_warmup_cosine_annealing
  scheduler_params:
    lambda_type: cosine
    warmup_factor: 0.2
    warmup_epochs: 5
    lr_min_factor: 0.01 

  max_epochs: 300
  energy_coefficient: 1
  loss_energy: mae
  # loss_force: l2mae

  eval_every: 1000


#slurm:
#  constraint: "volta32gb"